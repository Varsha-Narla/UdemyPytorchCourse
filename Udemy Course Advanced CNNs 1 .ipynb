{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40001536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5a49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a34dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1547bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing the mnist image files and converting them into tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6134f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a83d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:11<00:00, 826432.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 113529.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:04<00:00, 393257.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 3008611.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\DELL\\OneDrive\\Desktop\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.MNIST(root='C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fe0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=datasets.MNIST(root='C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c7208a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: C:\\Users\\DELL\\OneDrive\\Desktop\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7204a65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: C:\\Users\\DELL\\OneDrive\\Desktop\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c2126c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9968a469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5f6cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label=train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925c9e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bce345b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31cea481",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "train_loader=DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=500, shuffle=False)\n",
    "\n",
    "#batch_size is the nmber of records we process at a time. we set shuffle to true in training because sometimes, datasets are sorted,\n",
    "#the model might get loads of 0's first, followed by loads of 1's which is not what we want. \n",
    "#in the testing, we don't have to adjust weights, so we set the batch_size to a little higher\n",
    "#we also don't care what order the images come in, so there is no need to shuffle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e415566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "np.set_printoptions(formatter=dict(int=lambda x: f'{x:4}')) #formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first batch \n",
    "for images, labels in train_loader:\n",
    "    #60,000/100 == 60 times loop runs \n",
    "    break #because we only want the first batch \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18f293ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5eebb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1ec85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_sz=784, out_sz=10, layers=[120, 84]):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(in_sz, layers[0])\n",
    "        self.fc2=nn.Linear(layers[0], layers[1])\n",
    "        self.fc3=nn.Linear(layers[1], out_sz)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        X=F.relu(self.fc1(X))\n",
    "        X=F.relu(self.fc2(X))\n",
    "        X=self.fc3(X)\n",
    "        \n",
    "        return F.log_softmax(X, dim=1) #becuase it is a multi class classiication \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9a69467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "model=MultiLayerPerceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aff48c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94080\n",
      "120\n",
      "10080\n",
      "84\n",
      "840\n",
      "10\n"
     ]
    }
   ],
   "source": [
    " for param in model.parameters():\n",
    "        print(param.numel()) #making it print the number of parameters \n",
    "#in ann, if you sum up all those below, we have a huge number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50a03921",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3721a898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdc8035d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to combine that 1,28,28 into 784. \n",
    "images.view(100,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "610daf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 batch 200 loss:0.24509955942630768 accuracy:94.88\n",
      "Epoch 0 batch 400 loss:0.14002777636051178 accuracy:95.1075\n",
      "Epoch 0 batch 600 loss:0.07295873761177063 accuracy:95.38333333333334\n",
      "Epoch 1 batch 200 loss:0.08890343457460403 accuracy:96.575\n",
      "Epoch 1 batch 400 loss:0.06103847920894623 accuracy:96.6475\n",
      "Epoch 1 batch 600 loss:0.06553444266319275 accuracy:96.72666666666667\n",
      "Epoch 2 batch 200 loss:0.11172612011432648 accuracy:97.665\n",
      "Epoch 2 batch 400 loss:0.051425594836473465 accuracy:97.53\n",
      "Epoch 2 batch 600 loss:0.03153930604457855 accuracy:97.55\n",
      "Epoch 3 batch 200 loss:0.11145282536745071 accuracy:98.145\n",
      "Epoch 3 batch 400 loss:0.0499701052904129 accuracy:97.9775\n",
      "Epoch 3 batch 600 loss:0.05861545726656914 accuracy:97.98666666666666\n",
      "Epoch 4 batch 200 loss:0.007547399960458279 accuracy:98.35\n",
      "Epoch 4 batch 400 loss:0.040722399950027466 accuracy:98.4825\n",
      "Epoch 4 batch 600 loss:0.0627918541431427 accuracy:98.41166666666666\n",
      "Epoch 5 batch 200 loss:0.02982754074037075 accuracy:98.845\n",
      "Epoch 5 batch 400 loss:0.06590287387371063 accuracy:98.7\n",
      "Epoch 5 batch 600 loss:0.08722091466188431 accuracy:98.68666666666667\n",
      "Epoch 6 batch 200 loss:0.06697206944227219 accuracy:99.115\n",
      "Epoch 6 batch 400 loss:0.06968030333518982 accuracy:98.93\n",
      "Epoch 6 batch 600 loss:0.04523150622844696 accuracy:98.90333333333334\n",
      "Epoch 7 batch 200 loss:0.004118194803595543 accuracy:99.235\n",
      "Epoch 7 batch 400 loss:0.008266616612672806 accuracy:99.15\n",
      "Epoch 7 batch 600 loss:0.07855099439620972 accuracy:99.11\n",
      "Epoch 8 batch 200 loss:0.004976940806955099 accuracy:99.37\n",
      "Epoch 8 batch 400 loss:0.01220615766942501 accuracy:99.2675\n",
      "Epoch 8 batch 600 loss:0.02471695840358734 accuracy:99.24666666666667\n",
      "Epoch 9 batch 200 loss:0.02608617953956127 accuracy:99.455\n",
      "Epoch 9 batch 400 loss:0.016600891947746277 accuracy:99.375\n",
      "Epoch 9 batch 600 loss:0.001449562725611031 accuracy:99.27\n",
      "Duration: 2.874053374926249 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "#training \n",
    "epochs=10\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "train_correct=[]\n",
    "test_correct=[]\n",
    "\n",
    "for i in range (epochs):\n",
    "    trn_corr=0\n",
    "    tst_corr=0\n",
    "    for b, (X_train, y_train) in enumerate(train_loader): #b gives batch number, X_train gives the image, y_train gives the answer\n",
    "        b+=1\n",
    "        \n",
    "        y_pred=model(X_train.view(100,-1))\n",
    "        loss=criterion(y_pred, y_train)\n",
    "        predicted=torch.max(y_pred.data, 1)[1] #the output layer will actually give out probabilities for each number. we pick the one with the highest probability and the [1] will give the number \n",
    "        batch_corr=(predicted==y_train).sum()\n",
    "        trn_corr+=batch_corr\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if b%200==0:\n",
    "            acc=trn_corr.item()*100/(100*b)\n",
    "            print(f'Epoch {i} batch {b} loss:{loss.item()} accuracy:{acc}')\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "    \n",
    "    with torch.no_grad(): #no_grad here, not zero_grad because we don't want to update the weights in testing \n",
    "        for b,(X_test, y_test) in enumerate(test_loader):\n",
    "            y_val=model(X_test.view(500,-1))\n",
    "            predicted=torch.max(y_val.data,1)[1]\n",
    "            tst_corr+=(predicted==y_test).sum()\n",
    "            \n",
    "    loss=criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "total_time=time.time()-start_time\n",
    "print(f'Duration: {total_time/60} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "748a7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load_all=DataLoader(test_data, batch_size=10000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a200a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    for X_test, y_test in test_load_all:\n",
    "        y_val=model(X_test.view(len(X_test), -1))\n",
    "        predicted=torch.max(y_val, 1)[1]\n",
    "        correct+=(predicted==y_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb0dac81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.74"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*correct.item()/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2d32356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 973,    0,    3,    0,    2,    2,    8,    1,    3,    2],\n",
       "       [   0, 1130,    1,    0,    0,    0,    3,    5,    1,    2],\n",
       "       [   0,    3, 1004,    3,    2,    0,    3,    6,    4,    0],\n",
       "       [   2,    1,    3,  993,    0,   15,    1,    1,   15,    3],\n",
       "       [   1,    0,    2,    0,  958,    2,    3,    1,    3,   13],\n",
       "       [   0,    0,    0,    3,    1,  866,   10,    0,    5,    2],\n",
       "       [   1,    0,    2,    0,    4,    0,  930,    0,    1,    0],\n",
       "       [   1,    0,   11,    6,    4,    1,    0, 1009,    5,    7],\n",
       "       [   2,    1,    6,    2,    1,    5,    0,    1,  935,    4],\n",
       "       [   0,    0,    0,    3,   10,    1,    0,    4,    2,  976]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(predicted.view(-1), y_test.view(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
